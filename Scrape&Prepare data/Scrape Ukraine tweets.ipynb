{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tweepy.client import Client\n",
    "import tweepy\n",
    "import os\n",
    "import time\n",
    "from dateutil import parser\n",
    "\n",
    "bearer_token = \"FILL IN\"\n",
    "API_Key = \"FILL IN\"\n",
    "API_secret_key = \"FILL IN\"\n",
    "client = Client(bearer_token = bearer_token, \n",
    "                consumer_key = API_Key,\n",
    "                consumer_secret = API_secret_key,\n",
    "                wait_on_rate_limit = False,\n",
    "               return_type = dict)\n",
    "\n",
    "# check if the file exists in directory\n",
    "def check_file(filename):\n",
    "    if os.path.exists(filename):\n",
    "        #print(\"file found to update:\", filename)\n",
    "        return True\n",
    "    else:\n",
    "        #print(\"no file found, creating new file:\", filename)\n",
    "        return False\n",
    "\n",
    "# merge old and new data and save as csv\n",
    "def merge_data(new_data, old_data, filename):\n",
    "    merged_data = pd.concat([old_data, new_data])\n",
    "    merged_data.to_csv(filename, sep = \"|\")\n",
    "    return merged_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRAPE TWEETS BEFORE AND AFTER INVASION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# initiate start and end times\n",
    "start_before_invasion = '2022-02-23T02:30:00Z'\n",
    "invasion_time = '2022-02-24T02:30:00Z'\n",
    "start_time = '2022-02-24T03:00:00Z'\n",
    "end_after_invasion = '2022-02-25T02:30:00Z'\n",
    "\n",
    "for item in ['before', 'after']:\n",
    "    \n",
    "    if item == 'before':\n",
    "        print(\"Scraping tweets from before the invasion\")\n",
    "        \n",
    "        # initiate start and end time\n",
    "        start_time = start_before_invasion\n",
    "        end_time = invasion_time\n",
    "        \n",
    "        # initiate filenames\n",
    "        filename_tweets = 'before_invasion_tweets.csv'\n",
    "        filename_users = 'before_invasion_users.csv'\n",
    "        filename_places = 'before_invasion_places.csv'\n",
    "        \n",
    "    if item == 'after':\n",
    "        print(\"Scraping tweets from after the invasion\")\n",
    "        \n",
    "        # initiate start and end time\n",
    "        start_time = start_time\n",
    "        end_time = end_after_invasion\n",
    "        \n",
    "        # initiate filenames\n",
    "        filename_tweets = 'after_invasion_tweets.csv'\n",
    "        filename_users = 'after_invasion_users.csv'\n",
    "        filename_places = 'after_invasion_places.csv'\n",
    "        \n",
    "    tweets = client.search_all_tweets(query = '(Ukraine OR #Ukraine OR Russia OR #Russia OR Putin OR #Putin OR Zelensky OR #Zelensky OR #UkraineRussianWar) lang:en -is:reply -is:quote -is:retweet', expansions = [\"author_id\", \"geo.place_id\"], \n",
    "                           user_fields = [\"created_at\", \"public_metrics\", \"verified\"], \n",
    "                           tweet_fields = [\"author_id\", \"created_at\", \"public_metrics\", \"lang\", \"entities\", \"conversation_id\", \"referenced_tweets\"],\n",
    "                          place_fields = ['country'],\n",
    "                        start_time = start_time, end_time = end_time, max_results = 500)\n",
    "\n",
    "    # store dataframes\n",
    "    old_tweets = pd.json_normalize(tweets['data'])\n",
    "    old_tweets['text'] = old_tweets.apply(lambda row: row['text'].replace(\"|\", \"\"), axis = 1) # remove | from tweets\n",
    "\n",
    "    old_users = pd.json_normalize(tweets['includes']['users'])\n",
    "    old_users['username'] = old_users.apply(lambda row: row['username'].replace(\"|\", \"\"), axis = 1) # remove | from username\n",
    "    old_users['name'] = old_users.apply(lambda row: row['name'].replace(\"|\", \"\"), axis = 1) # remove | from name\n",
    "\n",
    "    if \"places\" in [item for item in tweets['includes']]:\n",
    "        old_places = pd.json_normalize(tweets['includes']['places'])\n",
    "\n",
    "    # go through \n",
    "    while ('next_token' in tweets['meta']):\n",
    "        \n",
    "        # print update\n",
    "        print(\"total number of tweets: \", len(old_tweets))\n",
    "\n",
    "        # sleep for 3 seconds to prevent reaching the rate limit\n",
    "        time.sleep(3)\n",
    "\n",
    "        next_token = tweets['meta']['next_token']\n",
    "        tweets = client.search_all_tweets(query = '(Ukraine OR #Ukraine OR Russia OR #Russia OR Putin OR #Putin OR Zelensky OR #Zelensky OR #UkraineRussianWar) lang:en -is:reply -is:quote -is:retweet', expansions = [\"author_id\", \"geo.place_id\"], \n",
    "                               user_fields = [\"created_at\", \"public_metrics\", \"verified\"], \n",
    "                               tweet_fields = [\"author_id\", \"created_at\", \"public_metrics\", \"lang\", \"entities\", \"conversation_id\", \"referenced_tweets\"],\n",
    "                              place_fields = ['country'],\n",
    "                            start_time = start_time, end_time = end_time, next_token = next_token, max_results = 500)\n",
    "\n",
    "        # update tweet csv\n",
    "        new_tweets = pd.json_normalize(tweets['data'])\n",
    "        new_tweets['text'] = new_tweets.apply(lambda row: row['text'].replace(\"|\", \"\"), axis = 1) # remove | from tweets\n",
    "        old_tweets = merge_data(new_tweets, old_tweets, filename_tweets)\n",
    "\n",
    "        # update users csv\n",
    "        new_users = pd.json_normalize(tweets['includes']['users'])\n",
    "        new_users['username'] = new_users.apply(lambda row: row['username'].replace(\"|\", \"\"), axis = 1) # remove | from username\n",
    "        new_users['name'] = new_users.apply(lambda row: row['name'].replace(\"|\", \"\"), axis = 1) # remove | from name\n",
    "        old_users = merge_data(new_users, old_users, filename_users)\n",
    "\n",
    "        # check if place\n",
    "        if \"places\" in [item for item in tweets['includes']]:\n",
    "            new_places = pd.json_normalize(tweets['includes']['places'])\n",
    "\n",
    "            if check_file(filename_places):\n",
    "                old_places = merge_data(new_places, old_places, filename_places)\n",
    "            else: \n",
    "                old_places = new_places\n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "print(\"time in sec elapsed: \", end_time - start_time)\n",
    "print(\"time in min elapsed: \", (end_time - start_time) / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRAPE EXTRA TWEETS BEFORE INVASION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# initiate start and end times\n",
    "start_time = '2022-02-21T02:30:00Z'\n",
    "end_time = '2022-02-23T02:30:00Z'\n",
    "\n",
    "# initiate filenames\n",
    "filename_tweets = 'before2_invasion_tweets.csv'\n",
    "filename_users = 'before2_invasion_users.csv'\n",
    "filename_places = 'before2_invasion_places.csv'\n",
    "\n",
    "\n",
    "tweets = client.search_all_tweets(query = '(Ukraine OR #Ukraine OR Russia OR #Russia OR Putin OR #Putin OR Zelensky OR #Zelensky OR #UkraineRussianWar) lang:en -is:reply -is:quote -is:retweet', expansions = [\"author_id\", \"geo.place_id\"], \n",
    "                       user_fields = [\"created_at\", \"public_metrics\", \"verified\"], \n",
    "                       tweet_fields = [\"author_id\", \"created_at\", \"public_metrics\", \"lang\", \"entities\", \"conversation_id\", \"referenced_tweets\"],\n",
    "                      place_fields = ['country'],\n",
    "                    start_time = start_time, end_time = end_time, max_results = 500)\n",
    "\n",
    "# store dataframes\n",
    "old_tweets = pd.json_normalize(tweets['data'])\n",
    "old_tweets['text'] = old_tweets.apply(lambda row: row['text'].replace(\"|\", \"\"), axis = 1) # remove | from tweets\n",
    "\n",
    "old_users = pd.json_normalize(tweets['includes']['users'])\n",
    "old_users['username'] = old_users.apply(lambda row: row['username'].replace(\"|\", \"\"), axis = 1) # remove | from username\n",
    "old_users['name'] = old_users.apply(lambda row: row['name'].replace(\"|\", \"\"), axis = 1) # remove | from name\n",
    "\n",
    "if \"places\" in [item for item in tweets['includes']]:\n",
    "    old_places = pd.json_normalize(tweets['includes']['places'])\n",
    "\n",
    "# go through \n",
    "while (('next_token' in tweets['meta']) and (len(old_tweets) < 900000)):\n",
    "\n",
    "    # print update\n",
    "    print(\"total number of tweets: \", len(old_tweets))\n",
    "\n",
    "    # sleep for 3 seconds to prevent reaching the rate limit\n",
    "    time.sleep(3)\n",
    "\n",
    "    next_token = tweets['meta']['next_token']\n",
    "    tweets = client.search_all_tweets(query = '(Ukraine OR #Ukraine OR Russia OR #Russia OR Putin OR #Putin OR Zelensky OR #Zelensky OR #UkraineRussianWar) lang:en -is:reply -is:quote -is:retweet', expansions = [\"author_id\", \"geo.place_id\"], \n",
    "                           user_fields = [\"created_at\", \"public_metrics\", \"verified\"], \n",
    "                           tweet_fields = [\"author_id\", \"created_at\", \"public_metrics\", \"lang\", \"entities\", \"conversation_id\", \"referenced_tweets\"],\n",
    "                          place_fields = ['country'],\n",
    "                        start_time = start_time, end_time = end_time, next_token = next_token, max_results = 500)\n",
    "\n",
    "    # update tweet csv\n",
    "    new_tweets = pd.json_normalize(tweets['data'])\n",
    "    new_tweets['text'] = new_tweets.apply(lambda row: row['text'].replace(\"|\", \"\"), axis = 1) # remove | from tweets\n",
    "    old_tweets = merge_data(new_tweets, old_tweets, filename_tweets)\n",
    "\n",
    "    # update users csv\n",
    "    new_users = pd.json_normalize(tweets['includes']['users'])\n",
    "    new_users['username'] = new_users.apply(lambda row: row['username'].replace(\"|\", \"\"), axis = 1) # remove | from username\n",
    "    new_users['name'] = new_users.apply(lambda row: row['name'].replace(\"|\", \"\"), axis = 1) # remove | from name\n",
    "    old_users = merge_data(new_users, old_users, filename_users)\n",
    "\n",
    "    # check if place\n",
    "    if \"places\" in [item for item in tweets['includes']]:\n",
    "        new_places = pd.json_normalize(tweets['includes']['places'])\n",
    "\n",
    "        if check_file(filename_places):\n",
    "            old_places = merge_data(new_places, old_places, filename_places)\n",
    "        else: \n",
    "            old_places = new_places\n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "print(\"time in sec elapsed: \", end_time - start_time)\n",
    "print(\"time in min elapsed: \", (end_time - start_time) / 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
