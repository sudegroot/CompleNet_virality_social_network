{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil import parser\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# initialise current year and month \n",
    "year = datetime.datetime.today().year\n",
    "print(\"current year: \", year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANALYSE TWEETS BEFORE INVASION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining Tweets and User files and splitting them into train, validation, and testset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tweet files\n",
    "before1 = pd.read_csv(\"before_invasion_tweets.csv\", sep = \"|\").drop(columns = ['Unnamed: 0'])\n",
    "before1['created_at'] = before1.apply(lambda row: parser.parse(row.created_at), axis = 1)\n",
    "before2 = pd.read_csv(\"before2_invasion_tweets.csv\", sep = \"|\").drop(columns = ['Unnamed: 0'])\n",
    "before2['created_at'] = before2.apply(lambda row: parser.parse(row.created_at), axis = 1)\n",
    "before_invasion = pd.concat([before1, before2])\n",
    "print(\"number of tweets before invasion: \", len(before_invasion))\n",
    "print(\"number of unique users based on tweets: \", len(before_invasion.author_id.unique()))\n",
    "\n",
    "# merge user files\n",
    "users_before1 = pd.read_csv(\"before_invasion_users.csv\", sep = \"|\").drop(columns = ['Unnamed: 0', 'withheld.country_codes', 'withheld.scope']).drop_duplicates(subset = ['id'], keep = 'first').rename(columns = {\"id\" : \"author_id\", \"created_at\" : \"account_age\"})\n",
    "users_before2 = pd.read_csv(\"before2_invasion_users.csv\", sep = \"|\").drop(columns = ['Unnamed: 0', 'withheld.country_codes', 'withheld.scope']).drop_duplicates(subset = ['id'], keep = 'first').rename(columns = {\"id\" : \"author_id\", \"created_at\" : \"account_age\"})\n",
    "users_before = pd.concat([users_before1, users_before2]).drop_duplicates(subset = ['author_id'], keep = 'first')\n",
    "users_before['account_age'] = users_before.apply(lambda row: parser.parse(row.account_age), axis = 1)\n",
    "users_before['account_age_y'] = users_before.apply(lambda row: year - row['account_age'].year, axis = 1)\n",
    "\n",
    "# combine tweets and users\n",
    "data_before = before_invasion.join(users_before.set_index('author_id'), on = ['author_id'])\n",
    "\n",
    "# make url boolean feature\n",
    "data_before['URL'] = data_before.apply(lambda row: 1 if pd.isnull(row['entities.urls']) == False else 0, axis = 1)\n",
    "data_before['retweeted'] = data_before.apply(lambda row: 1 if row['public_metrics.retweet_count'] > 0 else 0, axis = 1)\n",
    "\n",
    "# split data into train, validation, and test set\n",
    "before_train, rem = train_test_split(data_before, train_size = 0.7, random_state = 42, shuffle = True)\n",
    "before_validation, before_test = train_test_split(rem, test_size = 0.5, random_state = 32, shuffle = True)\n",
    "print(before_train.shape, before_validation.shape, before_test.shape)\n",
    "\n",
    "# save files to csv\n",
    "before_train.to_csv(\"before_train.csv\", sep = \"|\")\n",
    "before_validation.to_csv(\"before_val.csv\", sep = \"|\")\n",
    "before_test.to_csv(\"before_test.csv\", sep = \"|\")\n",
    "\n",
    "# show train set and continue with trainset for EDA\n",
    "before_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(before_train.author_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_before = before_train.drop(columns = ['geo.place_id', 'entities.mentions', 'withheld.copyright', 'withheld.country_codes', 'entities.cashtags', 'withheld.scope'])\n",
    "subset = data_before[['created_at', 'public_metrics.retweet_count',\n",
    "       'public_metrics.reply_count', 'public_metrics.like_count',\n",
    "       'public_metrics.quote_count', \n",
    "       'account_age', 'public_metrics.followers_count',\n",
    "       'public_metrics.following_count', 'public_metrics.tweet_count',\n",
    "       'public_metrics.listed_count', 'account_age_y']]\n",
    "# print(subset.describe().to_latex(caption = \"Data before invasion: description of numerical values.\"))\n",
    "subset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tweets = len(data_before)\n",
    "print(\"percentage of tweets retweeted\")\n",
    "print(\"never: \", len(data_before[data_before['public_metrics.retweet_count'] == 0]) / number_of_tweets * 100)\n",
    "print(\"more than 0: \", len(data_before[data_before['public_metrics.retweet_count'] > 0]) / number_of_tweets * 100)\n",
    "print(\"more than 1: \", len(data_before[data_before['public_metrics.retweet_count'] > 1]) / number_of_tweets * 100)\n",
    "print(\"more than 5: \", len(data_before[data_before['public_metrics.retweet_count'] > 5]) / number_of_tweets * 100)\n",
    "print(\"more than 10: \", len(data_before[data_before['public_metrics.retweet_count'] > 10]) / number_of_tweets * 100)\n",
    "print(\"more than 50: \", len(data_before[data_before['public_metrics.retweet_count'] > 50]) / number_of_tweets * 100)\n",
    "print(\"more than 100: \", len(data_before[data_before['public_metrics.retweet_count'] > 100]) / number_of_tweets * 100)\n",
    "\n",
    "plt.hist(data_before['public_metrics.retweet_count'], bins = 100, range = [5, 200])\n",
    "plt.xlabel(\"Number of retweets\", fontsize = 13)\n",
    "plt.ylabel(\"Count\", fontsize = 13)\n",
    "plt.savefig(\"retweet_count_before.jpg\")\n",
    "\n",
    "retweet_count = before_train['public_metrics.retweet_count'].sum()\n",
    "\n",
    "print(\"total number of retweets: \", retweet_count)\n",
    "print(\"percentage of total (original + its retweets): \", (retweet_count/(retweet_count + number_of_tweets)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANALYSE TWEETS AFTER INVASION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tweet files\n",
    "after_invasion = pd.read_csv(\"after_invasion_tweets.csv\", sep = \"|\").drop(columns = ['Unnamed: 0'])\n",
    "after_invasion['created_at'] = after_invasion.apply(lambda row: parser.parse(row.created_at), axis = 1)\n",
    "print(\"number of tweets before invasion: \", len(after_invasion))\n",
    "print(\"number of unique users based on tweets: \", len(after_invasion.author_id.unique()))\n",
    "\n",
    "# merge user files\n",
    "users_after = pd.read_csv(\"after_invasion_users.csv\", sep = \"|\").drop(columns = ['Unnamed: 0', 'withheld.country_codes', 'withheld.scope']).drop_duplicates(subset = ['id'], keep = 'first').rename(columns = {\"id\" : \"author_id\", \"created_at\" : \"account_age\"})\n",
    "users_after['account_age'] = users_after.apply(lambda row: parser.parse(row.account_age), axis = 1)\n",
    "users_after['account_age_y'] = users_after.apply(lambda row: year - row['account_age'].year, axis = 1)\n",
    "\n",
    "# combine tweets and users\n",
    "data_after = after_invasion.join(users_after.set_index('author_id'), on = ['author_id'])\n",
    "\n",
    "# make url boolean feature\n",
    "data_after['URL'] = data_after.apply(lambda row: 1 if pd.isnull(row['entities.urls']) == False else 0, axis = 1)\n",
    "data_after['retweeted'] = data_after.apply(lambda row: 1 if row['public_metrics.retweet_count'] > 0 else 0, axis = 1)\n",
    "\n",
    "# split data into train, validation, and test set\n",
    "after_train, rem_after = train_test_split(data_after, train_size = 0.7, random_state = 42, shuffle = True)\n",
    "after_validation, after_test = train_test_split(rem_after, test_size = 0.5, random_state = 32, shuffle = True)\n",
    "print(after_train.shape, after_validation.shape, after_test.shape)\n",
    "\n",
    "# save files to csv\n",
    "after_train.to_csv(\"after_train.csv\", sep = \"|\")\n",
    "after_validation.to_csv(\"after_val.csv\", sep = \"|\")\n",
    "after_test.to_csv(\"after_test.csv\", sep = \"|\")\n",
    "\n",
    "# show train set and continue with trainset for EDA\n",
    "after_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tweets = len(after_train)\n",
    "print(\"percentage of tweets retweeted\")\n",
    "print(\"never: \", len(after_train[after_train['public_metrics.retweet_count'] == 0]) / number_of_tweets * 100)\n",
    "print(\"more than 0: \", len(after_train[after_train['public_metrics.retweet_count'] > 0]) / number_of_tweets * 100)\n",
    "print(\"more than 1: \", len(after_train[after_train['public_metrics.retweet_count'] > 1]) / number_of_tweets * 100)\n",
    "print(\"more than 5: \", len(after_train[after_train['public_metrics.retweet_count'] > 5]) / number_of_tweets * 100)\n",
    "print(\"more than 10: \", len(after_train[after_train['public_metrics.retweet_count'] > 10]) / number_of_tweets * 100)\n",
    "print(\"more than 50: \", len(after_train[after_train['public_metrics.retweet_count'] > 50]) / number_of_tweets * 100)\n",
    "print(\"more than 100: \", len(after_train[after_train['public_metrics.retweet_count'] > 100]) / number_of_tweets * 100)\n",
    "\n",
    "plt.hist(after_train['public_metrics.retweet_count'], bins = 100, range = [5, 200])\n",
    "plt.xlabel(\"Number of retweets\", fontsize = 13)\n",
    "plt.ylabel(\"Count\", fontsize = 13)\n",
    "plt.savefig(\"retweet_count_after.jpg\")\n",
    "\n",
    "retweet_count = after_train['public_metrics.retweet_count'].sum()\n",
    "\n",
    "print(\"total number of retweets: \", retweet_count)\n",
    "print(\"percentage of total (original + its retweets): \", (retweet_count/(retweet_count + number_of_tweets)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
