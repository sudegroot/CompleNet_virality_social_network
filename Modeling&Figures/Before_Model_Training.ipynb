{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from functions_thesis import preprocessing, get_f1_macro, cross_validation_train, best_resampling\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "before_data = pd.read_csv(\"before_train_val.csv\", sep = \"|\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "# do some preprocessing\n",
    "before_data = preprocessing(before_data)\n",
    "print(before_data.shape)\n",
    "before_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BASELINE MODELS**\n",
    "\n",
    "We start with some baseline models, from here models will be improved using hyperparameter tuning and dealing with class imbalance. As a super simple baseline model we take a model that just predicts the majority class 'non-viral'. Moreover we use the F1 macro measurement to describe model performance: we assign equal weights to the F1 score of the majority and minority classes. \n",
    "We will test the following models: \n",
    "- Logistic model \n",
    "- Random Forest classifier\n",
    "- XGBoost classifier\n",
    "- Neural Network classifier\n",
    "\n",
    "For the non-tree based models, data will be scaled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before invasion models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "b_X = before_data[['verified', 'log_followers',\n",
    "       'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', \n",
    "       'sex_generalized', 'tweet_char_len', \n",
    "        'hashtag_count',\n",
    "       'mention_count', 'urls_count', 'organization', 'sentiment', 'emoji_count', 'public_metrics.retweet_count']]\n",
    "\n",
    "b_Y = before_data['viral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Majority model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_majority = baseline_model(a_X, a_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = LogisticRegression(random_state = 42) # BALANCED MADE PERFORMANCE WORSE\n",
    "resample = False\n",
    "scale = True \n",
    "\n",
    "# get evaluation\n",
    "metrics_LR_base, importances_LR_base = cross_validation_train(model, b_X, b_Y, resample, scale)\n",
    "get_f1_macro(metrics_LR_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = LogisticRegression(random_state = 42) # BALANCED MADE PERFORMANCE WORSE\n",
    "\n",
    "resampling_methods = {'RUS' : RandomUnderSampler(random_state = 42), 'ROS' : RandomOverSampler(random_state = 42), 'SMOTE' : SMOTE(random_state = 42, n_jobs = 3), 'bound' : list(np.arange(0, 65, 5))}\n",
    "scaler = True\n",
    "\n",
    "best_scores_mean_LR, best_scores_std_LR, best_ratio_LR = best_resampling(model, b_X, b_Y, resampling_methods, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = RandomForestClassifier(n_estimators = 100, random_state = 42, n_jobs = 3)\n",
    "resample = False\n",
    "scale = False\n",
    "\n",
    "# get evaluation\n",
    "metrics_RF_base, importances_RF_base = cross_validation_train(model, b_X, b_Y, resample, scale)\n",
    "get_f1_macro(metrics_RF_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = RandomForestClassifier(n_estimators = 100, random_state = 42, n_jobs = 3)\n",
    "\n",
    "resampling_methods = {'RUS' : RandomUnderSampler(random_state = 42), 'ROS' : RandomOverSampler(random_state = 42), 'SMOTE' : SMOTE(random_state = 42, n_jobs = 3), 'bound' : list(np.arange(0, 65, 5))}\n",
    "scaler = False\n",
    "\n",
    "best_scores_mean_RF, best_scores_std_RF, best_ratio_RF = best_resampling(model, b_X, b_Y, resampling_methods, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = XGBClassifier(n_estimators = 100, random_state = 42, n_jobs = 3)\n",
    "resample = False\n",
    "scale = False\n",
    "\n",
    "# get evaluation\n",
    "metrics_XG_base, importances_XG_base = cross_validation_train(model, b_X, b_Y, resample, scale)\n",
    "get_f1_macro(metrics_XG_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = XGBClassifier(n_estimators = 100, random_state = 42, n_jobs = 3)\n",
    "\n",
    "resampling_methods = {'RUS' : RandomUnderSampler(random_state = 42), 'ROS' : RandomOverSampler(random_state = 42), 'SMOTE' : SMOTE(random_state = 42, n_jobs = 3), 'bound' : list(np.arange(0, 65, 5))}\n",
    "scaler = False\n",
    "\n",
    "best_scores_mean_XGB, best_scores_std_XGB, best_ratio_XGB = best_resampling(model, b_X, b_Y, resampling_methods, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = MLPClassifier(random_state = 42)\n",
    "resample = False\n",
    "scale = True\n",
    "\n",
    "# get evaluation\n",
    "metrics_MLP_base, importances_MLP_base = cross_validation_train(model, b_X, b_Y, resample, scale)\n",
    "get_f1_macro(metrics_MLP_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = MLPClassifier(random_state = 42)\n",
    "\n",
    "resampling_methods = {'RUS' : RandomUnderSampler(random_state = 42), 'ROS' : RandomOverSampler(random_state = 42), 'SMOTE' : SMOTE(random_state = 42, n_jobs = 3), 'bound' : list(np.arange(0, 65, 5))}\n",
    "scaler = True\n",
    "\n",
    "best_scores_mean_MLP, best_scores_std_MLP, best_ratio_MLP = best_resampling(model, b_X, b_Y, resampling_methods, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HYPERPARAMETER TUNING BEST MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "ind_bound = b_X.index[(b_X['public_metrics.retweet_count'] >= 25) & (b_X['public_metrics.retweet_count'] <= 100)].tolist()\n",
    "len(ind_bound)\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "skf.get_n_splits(b_X, b_Y)\n",
    "\n",
    "cv = list()\n",
    "\n",
    "for item in skf.split(b_X, b_Y):\n",
    "    cv.append([np.array(list((set(item[0]) - set(ind_bound)))), item[1]])\n",
    "\n",
    "\n",
    "X_ = b_X.drop(columns = ['public_metrics.retweet_count'])\n",
    "    \n",
    "# do grid search \n",
    "model = RandomForestClassifier(random_state = 42, n_jobs = 2)\n",
    "\n",
    "grid = {\"n_estimators\" : [90, 100, 130], \n",
    "        \"criterion\" : ['gini', 'entropy'],\n",
    "        \"max_depth\" : [5, 10, 20, 40, 'None'],\n",
    "        \"min_samples_split\" : [2, 5, 10], \n",
    "        \"max_features\" : ['sqrt', 'None']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = grid, n_jobs = 2, cv = cv, scoring = 'f1_macro', refit = False)\n",
    "grid_result = grid_search.fit(X_, b_Y)\n",
    "\n",
    "mean = pd.DataFrame(grid_result.cv_results_).iloc[grid_result.best_index_]['mean_test_score']\n",
    "std = pd.DataFrame(grid_result.cv_results_).iloc[grid_result.best_index_]['std_test_score']\n",
    "\n",
    "print(\"mean score: %f +- %f\" % (mean, std))\n",
    "print(\"best parameters: \", grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best parameters**\n",
    "\n",
    "mean score: 0.751789 +- 0.003823\n",
    "\n",
    "best parameters:  {'criterion': 'entropy', 'max_depth': 40, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 130}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINAL MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = RandomForestClassifier(criterion = 'entropy', max_depth = 40, max_features = 'sqrt', min_samples_split = 5, n_estimators = 150, random_state = 42, n_jobs = 3)\n",
    "resample = 25\n",
    "scale = False\n",
    "\n",
    "# get evaluation\n",
    "metrics_RF_final, importances_RF_final = cross_validation_train(model, b_X, b_Y, resample, scale)\n",
    "get_f1_macro(metrics_RF_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
