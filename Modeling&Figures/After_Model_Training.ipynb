{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import math\n",
    "import numbers\n",
    "from dateutil import parser\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "import collections\n",
    "import time\n",
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "from emoji import UNICODE_EMOJI\n",
    "import random\n",
    "from functions_thesis import preprocessing, get_f1_macro, cross_validation_train, best_resampling\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "after_data = pd.read_csv(\"after_train_val.csv\", sep = \"|\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "# do some preprocessing\n",
    "after_data = preprocessing(after_data)\n",
    "print(after_data.shape)\n",
    "after_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BASELINE MODELS**\n",
    "\n",
    "We start with some baseline models, from here models will be improved using hyperparameter tuning and dealing with class imbalance. As a super simple baseline model we take a model that just predicts the majority class 'non-viral'. Moreover we use the F1 macro measurement to describe model performance: we assign equal weights to the F1 score of the majority and minority classes. \n",
    "We will test the following models: \n",
    "- Logistic model \n",
    "- Random Forest classifier\n",
    "- XGBoost classifier\n",
    "- Neural Network classifier\n",
    "\n",
    "For the non-tree based models, data will be scaled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After invasion models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "a_X = after_data[['verified', 'log_followers',\n",
    "       'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', \n",
    "       'sex_generalized', 'tweet_char_len', \n",
    "        'hashtag_count',\n",
    "       'mention_count', 'urls_count', 'organization', 'sentiment', 'emoji_count', 'public_metrics.retweet_count']]\n",
    "\n",
    "a_Y = after_data['viral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we resample the after data to match the same number as instances as the before data. This is to make both models comparable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly undersample data\n",
    "sample = {0: 586497, 1: 6376}\n",
    "resample = RandomUnderSampler(random_state = 42, sampling_strategy = sample)\n",
    "a_X, a_Y = resample.fit_resample(a_X, a_Y)\n",
    "Counter(a_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Majority model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_majority = baseline_model(a_X, a_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# set up model \n",
    "model = LogisticRegression(random_state = 42) # BALANCED MADE PERFORMANCE WORSE\n",
    "resample = False\n",
    "scale = True \n",
    "\n",
    "# get evaluation\n",
    "metrics_LR_base, importances_LR_base = cross_validation_train(model, a_X, a_Y, resample, scale)\n",
    "get_f1_macro(metrics_LR_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = LogisticRegression(random_state = 42) # BALANCED MADE PERFORMANCE WORSE\n",
    "\n",
    "resampling_methods = {'RUS' : RandomUnderSampler(random_state = 42), 'ROS' : RandomOverSampler(random_state = 42), 'SMOTE' : SMOTE(random_state = 42, n_jobs = 3), 'bound' : list(np.arange(0, 65, 5))}\n",
    "scaler = True\n",
    "\n",
    "best_scores_mean_LR, best_scores_std_LR, best_ratio_LR = best_resampling(model, a_X, a_Y, resampling_methods, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = RandomForestClassifier(n_estimators = 100, random_state = 42, n_jobs = 3)\n",
    "resample = False\n",
    "scale = False\n",
    "\n",
    "# get evaluation\n",
    "metrics_RF_base, importances_RF_base = cross_validation_train(model, a_X, a_Y, resample, scale)\n",
    "get_f1_macro(metrics_RF_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = RandomForestClassifier(n_estimators = 100, random_state = 42, n_jobs = 3)\n",
    "\n",
    "resampling_methods = {'RUS' : RandomUnderSampler(random_state = 42), 'ROS' : RandomOverSampler(random_state = 42), 'SMOTE' : SMOTE(random_state = 42, n_jobs = 3), 'bound' : list(np.arange(0, 65, 5))}\n",
    "scaler = False\n",
    "\n",
    "best_scores_mean_RF, best_scores_std_RF, best_ratio_RF = best_resampling(model, a_X, a_Y, resampling_methods, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# set up model \n",
    "model = RandomForestClassifier(n_estimators = 100, random_state = 42, n_jobs = 3)\n",
    "resample = 10\n",
    "scale = False\n",
    "\n",
    "# get evaluation\n",
    "metrics_RF_b_test, importances_RF_b_test = cross_validation_train(model, a_X, a_Y, resample, scale)\n",
    "get_f1_macro(metrics_RF_b_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# set up model \n",
    "model = XGBClassifier(n_estimators = 100, random_state = 42, n_jobs = 3)\n",
    "resample = False\n",
    "scale = False\n",
    "\n",
    "# get evaluation\n",
    "metrics_XG_base, importances_XG_base = cross_validation_train(model, a_X, a_Y, resample, scale)\n",
    "get_f1_macro(metrics_XG_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = XGBClassifier(n_estimators = 100, random_state = 42, n_jobs = 3)\n",
    "\n",
    "resampling_methods = {'RUS' : RandomUnderSampler(random_state = 42), 'ROS' : RandomOverSampler(random_state = 42), 'SMOTE' : SMOTE(random_state = 42, n_jobs = 3), 'bound' : list(np.arange(0, 65, 5))}\n",
    "scaler = False\n",
    "\n",
    "best_scores_mean_XGB, best_scores_std_XGB, best_ratio_XGB = best_resampling(model, a_X, a_Y, resampling_methods, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# set up model \n",
    "model = MLPClassifier(random_state = 42)\n",
    "resample = False\n",
    "scale = True\n",
    "\n",
    "# get evaluation\n",
    "metrics_MLP_base, importances_MLP_base = cross_validation_train(model, a_X, a_Y, resample, scale)\n",
    "get_f1_macro(metrics_MLP_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = MLPClassifier(random_state = 42)\n",
    "\n",
    "resampling_methods = {'RUS' : RandomUnderSampler(random_state = 42), 'ROS' : RandomOverSampler(random_state = 42), 'SMOTE' : SMOTE(random_state = 42, n_jobs = 3), 'bound' : list(np.arange(0, 65, 5))}\n",
    "scaler = True\n",
    "\n",
    "best_scores_mean_MLP, best_scores_std_MLP, best_ratio_MLP = best_resampling(model, a_X, a_Y, resampling_methods, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINAL MODEL HYPERPARAMETER TUNING**\n",
    "\n",
    "First a class is made for the resample method boundary, to put it into the pipeline for gridsearch. Ideal boundary is set on 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "ind_bound = a_X.index[(a_X['public_metrics.retweet_count'] >= 10) & (a_X['public_metrics.retweet_count'] <= 100)].tolist()\n",
    "len(ind_bound)\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "skf.get_n_splits(a_X, a_Y)\n",
    "\n",
    "cv = list()\n",
    "\n",
    "for item in skf.split(a_X, a_Y):\n",
    "    cv.append([np.array(list((set(item[0]) - set(ind_bound)))), item[1]])\n",
    "\n",
    "\n",
    "X_ = a_X.drop(columns = ['public_metrics.retweet_count'])\n",
    "    \n",
    "# do grid search TEST\n",
    "\n",
    "model = RandomForestClassifier(random_state = 42, n_jobs = 2)\n",
    "\n",
    "grid = {\"n_estimators\" : [90, 100, 130], \n",
    "        \"criterion\" : ['gini', 'entropy'],\n",
    "        \"max_depth\" : [5, 10, 20, 40, 'None'],\n",
    "        \"min_samples_split\" : [2, 5, 10], \n",
    "        \"max_features\" : ['sqrt', 'None']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = grid, n_jobs = 2, cv = cv, scoring = 'f1_macro', refit = False)\n",
    "grid_result = grid_search.fit(X_, a_Y)\n",
    "\n",
    "mean = pd.DataFrame(grid_result.cv_results_).iloc[grid_result.best_index_]['mean_test_score']\n",
    "std = pd.DataFrame(grid_result.cv_results_).iloc[grid_result.best_index_]['std_test_score']\n",
    "\n",
    "print(\"mean score: %f +- %f\" % (mean, std))\n",
    "print(\"best parameters: \", grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best parameters are:**\n",
    "\n",
    "mean score: 0.735954 +- 0.004972\n",
    "\n",
    "best parameters:  {'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 130}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINAL MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = RandomForestClassifier(criterion = 'gini', max_depth = 20, max_features = 'sqrt', min_samples_split = 2, n_estimators = 150, random_state = 42, n_jobs = 3)\n",
    "resample = 10\n",
    "scale = False\n",
    "\n",
    "# get evaluation\n",
    "metrics_RF_final, importances_RF_final = cross_validation_train(model, a_X, a_Y, resample, scale)\n",
    "get_f1_macro(metrics_RF_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
